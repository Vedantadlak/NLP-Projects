{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"twitter_train.csv\") #10980 rows 12 cols\n",
    "test = pd.read_csv(\"twitter_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train['negativereason_gold'].nunique())\n",
    "# print(train['negativereason_gold'].value_counts(),\"\\n\")\n",
    "\n",
    "# print(x_test.negativereason_gold.nunique())\n",
    "# print(x_test.negativereason_gold.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['airline_sentiment_gold','name','tweet_id', 'retweet_count','tweet_created','user_timezone','tweet_coord','tweet_location']\n",
    "train.drop(drop_cols, axis = 1, inplace=True)\n",
    "test.drop(drop_cols, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "stops += list(punctuation)\n",
    "stops += ['flight','airline','flights','AA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {'ppl': 'people','cust':'customer','serv':'service','mins':'minutes','hrs':'hours','svc': 'service',\n",
    "           'u':'you','pls':'please'}\n",
    "\n",
    "train_index = train[~train.negativereason_gold.isna()].index\n",
    "test_index = test[~test.negativereason_gold.isna()].index\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    tweet = row.text\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) #remove links\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #remove usernames\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet) #remove additional whitespaces\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n",
    "    tweet = tweet.strip('\\'\"') #trim tweet\n",
    "    words = []\n",
    "    for word in tweet.split():\n",
    "#         if not hasNumbers(word):\n",
    "        if word.lower() not in stops:\n",
    "            if word in list(abbreviations.keys()):\n",
    "                words.append(abbreviations[word])\n",
    "            else:\n",
    "                words.append(word.lower())   \n",
    "    tweet = \" \".join(words)\n",
    "    tweet = \" %s %s\" % (tweet, row.airline)\n",
    "    row.text = tweet\n",
    "    if index in train_index:\n",
    "        row.text = \" %s %s\" % (row.text, row.negativereason_gold)\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    tweet = row.text\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) #remove links\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #remove usernames\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet) #remove additional whitespaces\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #replace #word with word\n",
    "    tweet = tweet.strip('\\'\"') #trim tweet\n",
    "    words = []\n",
    "    for word in tweet.split(): \n",
    "#         if not hasNumbers(word):\n",
    "        if word.lower() not in stops:\n",
    "            if word in list(abbreviations.keys()):\n",
    "                words.append(abbreviations[word])\n",
    "            else:\n",
    "                words.append(word.lower())\n",
    "    tweet = \" \".join(words)\n",
    "    tweet = \" %s %s\" % (tweet, row.airline)\n",
    "    row.text = tweet\n",
    "    if index in test_index:\n",
    "        row.text = \" %s %s\" % (row.text, row.negativereason_gold)\n",
    "\n",
    "del train['negativereason_gold']\n",
    "del test['negativereason_gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    row.text = deEmojify(row.text)\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    row.text = deEmojify(row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    words = row.text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not hasNumbers(word):\n",
    "            new_words.append(word)\n",
    "    row.text = \" \".join(new_words)\n",
    "    \n",
    "for index, row in test.iterrows():\n",
    "    words = row.text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if not hasNumbers(word):\n",
    "            new_words.append(word)\n",
    "    row.text = \" \".join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocab and data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(analyzer='word', max_features=3150, max_df = 0.8, ngram_range=(1,1))\n",
    "train_features= v.fit_transform(train.text)\n",
    "test_features=v.transform(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C = 2.1, solver='liblinear', multi_class='auto')\n",
    "clf.fit(train_features,train['airline_sentiment'])\n",
    "pred = clf.predict(test_features)\n",
    "with open('predictions_twitter.csv', 'w') as f:\n",
    "    for item in pred:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"linear\", C= 0.96 , gamma = 'scale')\n",
    "# clf = SVC(C = 1000, gamma = 0.001)\n",
    "clf.fit(train_features, train['airline_sentiment'])\n",
    "pred = clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions_twitter2.csv', 'w') as f: #less accurate\n",
    "    for item in pred:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions_twitter2.csv', 'w') as f: #less accurate\n",
    "    for item in pred:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
